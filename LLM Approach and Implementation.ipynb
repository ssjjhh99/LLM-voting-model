{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4f9e97",
   "metadata": {},
   "source": [
    "Approach and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502c4e8",
   "metadata": {},
   "source": [
    "Step 1: Import state csv into the program by using pandas dataframes\n",
    "If we are given 325 million synthetic individuals data:\n",
    "age(19) x gender(2) x ethnicity(6) x income(16) x MSA(2) x state(51) = 372096 total groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333ddc4",
   "metadata": {},
   "source": [
    "Step 2: Query LLM for Voting Probability\n",
    "You can create a function, so that each time when you want to put something in LLM, you call this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0271898",
   "metadata": {},
   "source": [
    "Step 3: randomly select one sample from each group/combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcb52c",
   "metadata": {},
   "source": [
    "Try the program by using variable_mapping.csv and AK.csv\n",
    "combine step1,2,3 together, the program should be similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def get_prompt(random_sample, question, candidates):\n",
    "    prompt = f\"If the person's info is: {random_sample}\\n\"\n",
    "    prompt += f\"Here is the list of candidates:\\n\"\n",
    "    for candidate in candidates:\n",
    "        prompt += f\"- {candidate.strip()}\\n\"\n",
    "\n",
    "    prompt += (\n",
    "    f\"Based on this person's info, what is the probability of the question: {question}\\n\"\n",
    "    \"Please give your answer in the following format (each on a new line):\\n\"\n",
    "    \"Candidate1: xx%\\n\"\n",
    "    \"Candidate2: xx%\\n\"\n",
    "    \"Candidate3: xx%\\n\"\n",
    "    \"...\\n\"\n",
    "    \"Only give probabilities for each candidate in order.\\n\"\n",
    ")\n",
    "    print (prompt) # don't need to print, here is for explanation\n",
    "    # Now this prompt is ready to be sent to the LLM\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# a function that whenever import a new state you can use it.\n",
    "def process_state(grouped, desc_cols, state_name):\n",
    "    question = input(\"Enter your question: \")\n",
    "    candidates = input(\"Enter ALL candidate names separated by commas: \").split(\",\")\n",
    "    for group_key, group_df in grouped:\n",
    "        # Randomly pick one row from this group (group_df)\n",
    "        if not group_df.empty:\n",
    "            random_row = group_df.sample(n=1).iloc[0]\n",
    "            demographic_info = ', '.join([str(random_row[col]) for col in desc_cols])\n",
    "            loc_desc = 'Small town' if random_row['loc_msa'] == 'S' else 'Large city'\n",
    "            new_prompt = f\"{demographic_info}, lives in {loc_desc}, in {state_name}\"\n",
    "            prompt = get_prompt(new_prompt, question, candidates) \n",
    "            # TODO: can call a new function called query_llm_api(prompt) to connect with LLM API\n",
    "\n",
    "    return\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    #Assume all data files in a folder called \"data\" next to this script\n",
    "    # When run the program, use a for loop to access each data file in \"data\" folder\n",
    "    df = pd.read_csv(\"data/AK.csv\")\n",
    "    vm = pd.read_csv(\"data/variable_mapping.csv\")\n",
    "    desc_cols = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        vm_subset = vm[vm['var_id'] == column]\n",
    "\n",
    "        if not vm_subset.empty:\n",
    "            value_dict = dict(zip(vm_subset['value_id'], vm_subset['var_values']))\n",
    "            description = vm_subset['description'].iloc[0].upper()\n",
    "            df[description] = df[column].map(value_dict)\n",
    "            desc_cols.append(description)\n",
    "            \n",
    "    group_keys = desc_cols + ['loc_msa']\n",
    "\n",
    "    grouped = df.groupby(group_keys)\n",
    "\n",
    "    process_state(grouped, desc_cols, 'Alaska')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b8b4e",
   "metadata": {},
   "source": [
    "we can pick one to see if this prompt can work and what does LLM return to this prompt (output hidden for brevity)\n",
    "choose prompt: \n",
    "\n",
    "If the person's info is: 18 to 19 years, Female, Population of one race: American Indian and Alaska Native, $10 000 to $14 999, lives in Large city, in Alaska\n",
    "Here is the list of candidates:\n",
    "- Joe Biden\n",
    "- Donald trump\n",
    "- Bill Clinton\n",
    "Based on this person's info, what is the probability that this person votes for each candidate?\n",
    "Please give your answer in the following format (each on a new line):\n",
    "Candidate1: xx%\n",
    "Candidate2: xx%\n",
    "Candidate3: xx%\n",
    "...\n",
    "Only give probabilities for each candidate in order.\n",
    "----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "since I don't have authority to use LLM API in GitHub, I can't really apply the actual API here, but the outline is:\n",
    "Create a new function (query_llm_api(prompt))\n",
    "query_llm_api(prompt) is a function that send prompt to the chosen API, and store the result of the reply of API then convert into string\n",
    "\n",
    "Sample string:\n",
    "Joe Biden: 45%\n",
    "\n",
    "Donald Trump: 50%\n",
    "\n",
    "Bill Clinton: 5%\n",
    "\n",
    "You call query_llm_api(prompt) inside of process_state(grouped, desc_cols, state_name) (This is where TODO)\n",
    "then extract probability number in the returned string\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "For example, this group has 3000 samples, and we get 3000 x 0.25 = 750 from LLM for voting for Biden, so 3000 x 0.65 = 1950 voting for Trump, and 3000 x 0.1 = 300 for Clinton\n",
    "Store the total amount of ppl voting for each candidate.\n",
    "You repeat those steps to get all groups in one state, then you move on to the next state.\n",
    "Finally, compare total amount of voting for each candidate and draw the conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e139a",
   "metadata": {},
   "source": [
    "Metric:\n",
    "\n",
    "To ensure the program works correctly, I checked (1) that prompts were generated for all groups, (2) all predicted probabilities were valid (0â€“1), (3) vote estimates were calculated as probability * group size, (4) total estimated votes were reasonable relative to population, and (5) sample prompts were reviewed for format accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb11f75",
   "metadata": {},
   "source": [
    "Update 07/12\n",
    "\n",
    "llm_voting_model.py is the file that accept other modules to import.\n",
    "political_llm.py is the file that has a class which includes method to build prompt, connect LLM and output results returned by LLM\n",
    "\n",
    "In this version, once the program starts, it automatically access state csv files inside of \"data\" folder. (I put AK.csv, VT.csv and WY.csv for testing). As I implement multiprocessing, all files may start process at the same time.\n",
    "\n",
    "API connection is also availble here:\n",
    "Key:BOtO6XUYDtqJ2hxmGjM18DN7AUI33nsJ8UAUsrtdh33vsvWIvFJtJQQJ99BGACHYHv6XJ3w3AAAAACOG0aWf\n",
    "URL: https://jiahu-mcviy2ug-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-35-turbo1/chat/completions?api-version=2025-01-01-preview\n",
    "Deployment Name:gpt-35-turbo1\n",
    "\n",
    "This version is able to connect to LLM and successfully get the result back.\n",
    "The program is able to visualize the progress, the user can see it in terminal.\n",
    "\n",
    "\n",
    "\n",
    "Testing:\n",
    "See \"Screenshot 2025-07-11 at 12.32.40 AM.png\", \"data\" folder has 3 state csv files, and the program predict Joe Biden is the winner with around 650776 votes while there are 1,532,743 ppl in total when I input \"Joe Biden, Bill Clinton, Donald Trump\" as three candidates. This result seems plausible if there are 3 candidates. When here are 3 state files, it takes about 81 minutes, while 1 files takes about 40-50 minutes, using multiprocessing did make the program faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
